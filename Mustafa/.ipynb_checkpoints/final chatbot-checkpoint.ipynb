{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8bbedf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from random import randrange\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('all')\n",
    "import wordnet\n",
    "nlp = spacy.load('en_core_web_lg',  disable=[\"parser\", \"ner\"])\n",
    "import io\n",
    "import random\n",
    "import string \n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf83bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "    \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "#print (type(response.json()))\n",
    "#print(response.json())\n",
    "df = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da3ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "playerid = 0\n",
    "goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "playercode = {'E. Haaland':0, 'H. Kane':1, 'A. Mitrović':2, 'I. Toney':3, 'L. Trossard':4, 'M. Almirón':5, 'P. Foden':6, 'Roberto Firmino':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea7939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b66aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askplayer(name):\n",
    "    askq = input(\"I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "    if askq =='yes' or askq == 'Yes':\n",
    "        print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "        print(\"E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\")  \n",
    "       #playerchoose = input(\"So what's your choice \")\n",
    "        while True:\n",
    "            playerchoose = input(\"So what's your choice \")\n",
    "            if playerchoose in playercode.keys():\n",
    "                playerid = (playercode[playerchoose])\n",
    "                #print(playerid)\n",
    "                print(\"hmm...\")\n",
    "                time.sleep(1)\n",
    "                print(\"Ah yes... \" + playerchoose + \" I know all about him. Did you know in this season he has scored a total of \" \n",
    "                      + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                break\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"That's not one of the names I gave you... Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d9de705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? no\n"
     ]
    }
   ],
   "source": [
    "askplayer(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c74ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec = spacy.load(\"en_core_web_sm\")\n",
    "# if on jupyter notebook open up the anaconda terminal and typeb-m spacy download en_core_web_lg, if it isn't installed\n",
    "#replace spacy.load(\"en\") with spacy.load(\"en_core_web_sm\")\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt')\n",
    "#python -m spacy download en   ...do this in the conda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40b467f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a = \"I've been watching football since I was two\"\n",
    "response_b = \"I hate Manchester City\"\n",
    "response_c = \"hello I like wednesdays. John anything today\"\n",
    "response_d = \"my favorite team is Manchester United\"\n",
    "response_e = \"I've been watching football since I was two\"\n",
    "blank_spot = \"player\"\n",
    "\n",
    "\n",
    "responses = [response_a, response_b, response_c, response_d, response_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "67924d16",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556bccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordIn(text):\n",
    "    hjk = list()\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            hjk.append(word[0])\n",
    "    return(hjk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15be9d47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'name',\n",
       " 'john',\n",
       " 'are',\n",
       " 'today',\n",
       " 'there',\n",
       " 'anything',\n",
       " 'like',\n",
       " 'do',\n",
       " 'wednesdays']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagofwords(\"hello my name is John how are you today is there anything you like to do on Wednesdays \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ba8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6334d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = [\"hello my name is John how are you today is there anything you like to do on Wednesdays\"]\n",
    "speech2 = word_tokenization(\"hello my name is John how are you today is there anything you like to do on Wednesdays. I do not like  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237b2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = [('hello', 'NN'), ('name', 'NN'), ('john', 'NN'), ('are', 'VBP'), ('today', 'NN'), ('there', 'RB'), ('anything', 'NN'), ('like', 'IN'), ('do', 'VBP'), ('wednesdays', 'NNS'), ('do', 'VB'), ('like', 'IN'), ('wednesdays', 'NNS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869ef1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a49e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    qans = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    qans2 = input(listintro[randrange(4)])\n",
    "    for word in exit_command:\n",
    "        if word in qans:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in qans:\n",
    "        askplayer(\"example\")\n",
    "        if askplayer\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a8480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a5d28df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215afca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "19f70cff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_respone = word_tokenization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8b7baa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_response = pos_tag(tokenized_respone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tagged_response:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3af2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace53149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f825a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209358bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896aa629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295261bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a1ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9fff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba033e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cad24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = 'Python is the greatest language in the world'\n",
    "def \n",
    "doc = nlp(my_str)\n",
    "words_lemmas_list = [token.lemma_ for token in doc]\n",
    "print(words_lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73256710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizingword(text):\n",
    "    doc = nlp(text)\n",
    "    words_lemmas_list = [token.lemma_ for token in doc]\n",
    "    return(words_lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd786819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e296657",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f76a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab747cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemTokens(tokens):\n",
    "    return [ lemmatizingword(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ada467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cae48468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'be', 'the', 'great', 'language', 'in', 'the', 'world']\n"
     ]
    }
   ],
   "source": [
    "# keeping only tagger component needed for lemmatization\n",
    "my_str = 'Python is the greatest language in the world'\n",
    "\n",
    "doc = nlp(my_str)\n",
    "words_lemmas_list = [token.lemma_ for token in doc]\n",
    "print(words_lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1dcd9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacypreprocessing(ik):\n",
    "    value = nlp(ik)\n",
    "    return [token.lemma_ for token in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f479bd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'my', 'name', 'is']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sent_tokens = nltk.sent_tokenize(responses)\n",
    "#word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ef69d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatresponse(text):\n",
    "    lower_response = [x.lower() for x in responses]\n",
    "    word_tokenization(text)\n",
    "    tvec = TfidfVectorizer(tokenizer = token.lemma_, stop_words='english')\n",
    "    tfidf = tvec.fit_transform(word_tokenization)\n",
    "    vals = cosine_similarity(tfidf[-1],tvec)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b30ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e35a9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "226f578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:/Users/Mustafa/anaconda3/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\",'C:/Users/Mustafa/anaconda3/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d6042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674b540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3e67b803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9c46983a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i like manchester city.\\ni hate manchester united.\\ni've been watching football since i was two.\\n\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aca80df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "67374184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacypreprocessing(ik):\n",
    "    value = nlp(ik)\n",
    "    return [token.lemma_ for token in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "98c7a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c4438d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e5e37ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=open('responses.txt','r',errors = 'ignore')\n",
    "resp1 =k1.read()\n",
    "resp1 = resp1.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's what I need to do. First I need to append the user_input into the corpora and then I need to vectorize it.\n",
    "#after that I need to use cosine similarity on [-1] and the rest of the list\n",
    "#use argsort to select the [0][-2] from the array. this selects the one before the highest matching array\n",
    "#use flatten on the array. then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "dcfa2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',resp1)\n",
    "    text_tokenized = word_tokenize(resp1)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)\n",
    "    \n",
    "    \n",
    "    \n",
    "def sentence_tokenization(text):\n",
    "    nltk.sent_tokenize(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f537008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ea132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ba8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4db715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
