{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8bbedf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from random import randrange\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('all')\n",
    "import wordnet\n",
    "nlp = spacy.load('en_core_web_lg',  disable=[\"parser\", \"ner\"])\n",
    "import io\n",
    "import random\n",
    "import string \n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag\n",
    "wnl = WordNetLemmatizer()\n",
    "warnings.filterwarnings('ignore')\n",
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf83bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "    \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "#print (type(response.json()))\n",
    "#print(response.json())\n",
    "df = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "da3ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "playerid = 0\n",
    "goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "playercode = {'Earling Haaland':0, ' Harry Kane':1, 'Aleksandar Mitrovic':2, 'Ivan Toney':3, 'Leandro Trossard':4, ' M. Miguel Almiron':5, 'Phil Foden':6, 'Roberto Firmino':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "0b66aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askplayer(name):\n",
    "    askq = input(\"If you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "    if askq =='yes' or askq == 'Yes':\n",
    "        print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "        print(\"Earling Haaland, Harry Kane, Aleksandar Mitrovic, Ivan Toney, Leandro Trossard, M. Miguel Almiron, Phil Foden, Roberto Firmino\")  \n",
    "       #playerchoose = input(\"So what's your choice \")\n",
    "        while True:\n",
    "            playerchoose = input(\"So what's your choice \")\n",
    "            if playerchoose in playercode.keys():\n",
    "                playerid = (playercode[playerchoose])\n",
    "                #print(playerid)\n",
    "                print(\"hmm...\")\n",
    "                time.sleep(1)\n",
    "                print(\"Ah yes... \" + playerchoose + \" I know all about him. Did you know in this season he has scored a total of \" \n",
    "                      + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                #return False\n",
    "                break\n",
    "            else:\n",
    "                print(\"That's not one of the names I gave you... Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5093c29",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "d0781d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1] or 'JJ' in word[1]  or 'VB' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in entity_list:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "d76a70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_response_joined(text):\n",
    "    global lemlist\n",
    "    number = 0\n",
    "    lemlist = []\n",
    "    n=0\n",
    "    for word in   text:\n",
    "        number = number+1\n",
    "     \n",
    "    while n<number:\n",
    "        gg =' '.join(text[n])\n",
    "        lemlist.append(gg)\n",
    "        n=n+1\n",
    "    \n",
    "    #print(lemlist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea43d66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_respone = word_tokenization(resp1)\n",
    "tagged_response = pos_tag(tokenized_respone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "28d61793",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [\" \",\n",
    "            \"hello, hey, hi, how are you\",\n",
    "            \"what is your favourite team\",\n",
    "            \"what is your least favourite or most hated team\",\n",
    "            \"how long have you been watching football\"\n",
    "                    \n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "a2b8f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = [\"Ask another question\",\n",
    "            'Hey, my name is Garry I will be answering your football questions, feel free to ask anything',\n",
    "                  'My favourite team is Manchester United',\n",
    "                  'I really hate Manchester City',\n",
    "                 \"I've been watching football since I was two, for as long as I remember I've loved football\",    \n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "1d1730e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = []\n",
    "for word in question:\n",
    "    question_list.append(lemmatize_sentence(word,))\n",
    "    \n",
    "lem_response_joined(question_list)\n",
    "question_lem_list = lemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "5baefafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_frame = pd.DataFrame()\n",
    "chat_frame['question'] = question_lem_list\n",
    "chat_frame['response'] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "02074138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "b31e9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_selection(ui):\n",
    "    sentence_input = [(ui)]\n",
    "    similarity_index_list = cosine_similarity(v.fit_transform(chat_frame[\"question\"]), v.transform(sentence_input)).flatten()\n",
    "    output = chat_frame.loc[similarity_index_list.argmax(), \"response\"]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "98c49d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    tag = True\n",
    "    #exit_command = ['bye','exit,','goodbye']\n",
    "    while tag == True:\n",
    "        user_input = input()\n",
    "        user_input = user_input.lower()\n",
    "        if user_input == \"bye\" or  user_input == \"exit\" or  user_input == \"goodbye\":\n",
    "            print(\"goodbye\")\n",
    "            tag = False\n",
    "        \n",
    "        elif \"top scorer\" in user_input:\n",
    "            askplayer(\"name\")\n",
    "            \n",
    "        else:\n",
    "            tokenized_respone = word_tokenization(user_input)\n",
    "            tagged_response = pos_tag(tokenized_respone)\n",
    "            taggandlem(tagged_response)\n",
    "            user_input = lmt_string\n",
    "            #print(user_input)\n",
    "            response_selection(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "f18d2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you doing\n",
      "Ask another question\n",
      "bye\n",
      "goodbye\n"
     ]
    }
   ],
   "source": [
    "speech(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "7e2824b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you doing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'are doing'"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_respone = word_tokenization(input())\n",
    "tagged_response = pos_tag(tokenized_respone)\n",
    "taggandlem(tagged_response)\n",
    "lmt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ae030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278283e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fbf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa5606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f76b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7edc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20547256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251541f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1f216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30c829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a347d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbcf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576127b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee57de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788de20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca7447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3a88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77b2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4db715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "60d7e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer()\n",
    "sentence_input = [\"hello\"]\n",
    "similarity_index_list = cosine_similarity(v.fit_transform(data[\"question\"]), v.transform(sentence_input)).flatten()\n",
    "output = data.loc[similarity_index_list.argmax(), \"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7935daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        if word in responses_option:\n",
    "            frequency +=1\n",
    "            n=n+1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)\n",
    "\n",
    "def frequency_of_terms(user_input,responses_option,n):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    \n",
    " #   user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        if word in responses_option:\n",
    "            frequency +=1\n",
    "            \n",
    "   # if n<3\n",
    "    frequency_of_terms(user_input,responses_option,(1))\n",
    "            \n",
    "            \n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)\n",
    "    \n",
    "    \n",
    "    \n",
    "def frequency_of_terms(user_input,response_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in response_option:\n",
    "            if response_option[n] <response_option[3]:\n",
    "                frequency +=1\n",
    "                if response_option[n] <= response_option[-1]:\n",
    "                    n=n+1\n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)\n",
    "    \n",
    "def frequency_of_terms(user_input,responses_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in responses_option:\n",
    "            if word in item[n]:\n",
    "                frequency +=1\n",
    "                n=n+1\n",
    "    \n",
    "   # while n<3:\n",
    "    #    frequency_of_terms(user_input,responses_option)\n",
    "        \n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)\n",
    "\n",
    "def frequency_of_terms(user_input,responses_option):\n",
    "    frequency = 0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in responses_option[n] :\n",
    "            if word in item:\n",
    "                if responses_option[:]\n",
    "    print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    #lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "    for word in lemmatized_words:\n",
    "        if word in stopwords.words('english'):\n",
    "            lemmatized_words.remove(word)\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfinished\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    tokenized_respone = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_respone)\n",
    "    taggandlem(tagged_response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea232a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "\n",
    "    for lit in entity_list:\n",
    "        print(wnl.lemmatize(lit))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''=\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    mis_undlist = [\"I am sorry! I don't understand you\",\"I didn't quite get that\",\"I'm sorry can you repeat that\",\"I misunderstood that\",\"I'm sorry what\"]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+mis_undlist[randrange(4)]\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb274b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    \n",
    "    for word in exit_command:\n",
    "        if word in userinput:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in userinput:\n",
    "        askplayer(\"example\")\n",
    "        \n",
    "    \n",
    "    userinput2 = input(listintro[randrange(4)])\n",
    "    \n",
    "    \n",
    "def speech(text):\n",
    "    userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    sent_tokens = nltk.sent_tokenize(resp1)\n",
    "    word_tokens = nltk.word_tokenize(resp1)\n",
    "    lmt_string = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for word in exit_command:\n",
    "        if word in userinput:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "            exit()\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in userinput:\n",
    "        askplayer(\"example\")\n",
    "        \n",
    "    \n",
    "    userinput2 = input(listintro[randrange(4)])\n",
    "    \n",
    "def speech(text):\n",
    "        tag = True\n",
    "        exit_command = ['bye','exit,','goodbye']\n",
    "\n",
    "        user_input = input(\"type in something\")\n",
    "        for word in user_input.lower().split():\n",
    "            if word not in exit_command:\n",
    "                #print(\"yaaaa\")\n",
    "                sentence_input = [user_input]\n",
    "                similarity_index_list = cosine_similarity(v.fit_transform(data[\"question\"]), v.transform(sentence_input)).flatten()\n",
    "                output = data.loc[similarity_index_list.argmax(), \"answer\"]\n",
    "                print(output)\n",
    "\n",
    "        \n",
    "            else:\n",
    "                print(\"goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "lmt_corp =  nltk.sent_tokenize(lemmatization(resp1))\n",
    "#may have to reset sent_token every time\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4e5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f8736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32187324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_chat(tile):\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    for word in exit_command:\n",
    "        if word in tile:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307621b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "  #  for word in text:\n",
    "   #     if 'NN' in word[1]:\n",
    "    #        entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in text:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "bbb55903",
   "metadata": {},
   "outputs": [],
   "source": [
    "elist = []\n",
    "for word in sent_tokens:\n",
    "    elist.append(lemmatize_sentence(word,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cf22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)\n",
    "\n",
    "\n",
    "def taggandlem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in entity_list:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
