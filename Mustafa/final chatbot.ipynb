{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8bbedf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from random import randrange\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('all')\n",
    "import wordnet\n",
    "nlp = spacy.load('en_core_web_lg',  disable=[\"parser\", \"ner\"])\n",
    "import io\n",
    "import random\n",
    "import string \n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf83bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "    \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "#print (type(response.json()))\n",
    "#print(response.json())\n",
    "df = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da3ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "playerid = 0\n",
    "goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "playercode = {'E. Haaland':0, 'H. Kane':1, 'A. Mitrović':2, 'I. Toney':3, 'L. Trossard':4, 'M. Almirón':5, 'P. Foden':6, 'Roberto Firmino':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea7939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b66aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askplayer(name):\n",
    "    askq = input(\"I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "    if askq =='yes' or askq == 'Yes':\n",
    "        print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "        print(\"E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\")  \n",
    "       #playerchoose = input(\"So what's your choice \")\n",
    "        while True:\n",
    "            playerchoose = input(\"So what's your choice \")\n",
    "            if playerchoose in playercode.keys():\n",
    "                playerid = (playercode[playerchoose])\n",
    "                #print(playerid)\n",
    "                print(\"hmm...\")\n",
    "                time.sleep(1)\n",
    "                print(\"Ah yes... \" + playerchoose + \" I know all about him. Did you know in this season he has scored a total of \" \n",
    "                      + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                break\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"That's not one of the names I gave you... Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d9de705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? no\n"
     ]
    }
   ],
   "source": [
    "askplayer(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c74ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec = spacy.load(\"en_core_web_sm\")\n",
    "# if on jupyter notebook open up the anaconda terminal and typeb-m spacy download en_core_web_lg, if it isn't installed\n",
    "#replace spacy.load(\"en\") with spacy.load(\"en_core_web_sm\")\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt')\n",
    "#python -m spacy download en   ...do this in the conda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40b467f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a = \"I've been watching football since I was two\"\n",
    "response_b = \"I hate Manchester City\"\n",
    "response_c = \"hello I like wednesdays. John anything today\"\n",
    "response_d = \"my favorite team is Manchester United\"\n",
    "response_e = \"I've been watching football since I was two\"\n",
    "blank_spot = \"player\"\n",
    "\n",
    "\n",
    "responses = [response_a, response_b, response_c, response_d, response_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "67924d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556bccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordIn(text):\n",
    "    hjk = list()\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            hjk.append(word[0])\n",
    "    return(hjk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15be9d47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'name',\n",
       " 'john',\n",
       " 'are',\n",
       " 'today',\n",
       " 'there',\n",
       " 'anything',\n",
       " 'like',\n",
       " 'do',\n",
       " 'wednesdays']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagofwords(\"hello my name is John how are you today is there anything you like to do on Wednesdays \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6334d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = [\"hello my name is John how are you today is there anything you like to do on Wednesdays\"]\n",
    "speech2 = word_tokenization(\"hello my name is John how are you today is there anything you like to do on Wednesdays. I do not like  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237b2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = [('hello', 'NN'), ('name', 'NN'), ('john', 'NN'), ('are', 'VBP'), ('today', 'NN'), ('there', 'RB'), ('anything', 'NN'), ('like', 'IN'), ('do', 'VBP'), ('wednesdays', 'NNS'), ('do', 'VB'), ('like', 'IN'), ('wednesdays', 'NNS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a49e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    qans = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    qans2 = input(listintro[randrange(4)])\n",
    "    for word in exit_command:\n",
    "        if word in qans:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in qans:\n",
    "        askplayer(\"example\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "1d7a8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization2(entity):\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(entity)]\n",
    "    return(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "460f294f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a831a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=open('responses.txt','r',errors = 'ignore')\n",
    "resp1 =k1.read()\n",
    "resp1 = resp1.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b5b7b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a7c3c9d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_respone = word_tokenization(resp1)\n",
    "tagged_response = pos_tag(tokenized_respone)\n",
    "lemaitized_response = lemmatization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "1213ede2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city', 'hate', 'manchester', 'watching', 'football']"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list = []\n",
    "for word in tagged_response:\n",
    "    if 'NN' in word[1]:\n",
    "        entity_list.append(word[0])\n",
    "        \n",
    "entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "a7f7e34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching'"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize(\"watching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ac836f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "hate\n",
      "manchester\n",
      "watching\n",
      "football\n",
      "cactus\n",
      "octopus\n"
     ]
    }
   ],
   "source": [
    "for word in entity_list:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text)\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "\n",
    "    for l in entity_list:\n",
    "        print(wnl.lemmatize(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a3529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e598c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff425d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "433153ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    #lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "    for word in lemmatized_words:\n",
    "        if word in stopwords.words('english'):\n",
    "            lemmatized_words.remove(word)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "306fa2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like manchester city\\ni hate manchester united\\nive been watching football since i was two\\n'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "08760ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'like',\n",
       " 'manchester',\n",
       " 'city',\n",
       " '.',\n",
       " '\\n',\n",
       " 'I',\n",
       " 'hate',\n",
       " 'manchester',\n",
       " 'united',\n",
       " '.',\n",
       " '\\n',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'be',\n",
       " 'watch',\n",
       " 'football',\n",
       " 'since',\n",
       " 'I',\n",
       " 'be',\n",
       " 'two',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization2(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "27589e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ['like',\n",
    " 'manchester',\n",
    " 'city',\n",
    " 'hate',\n",
    " 'manchester',\n",
    " 'united',\n",
    " 'ive',\n",
    " 'watching',\n",
    " 'football',\n",
    " 'since',\n",
    " 'was',\n",
    " 'two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7adfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12639e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6424ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9054e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's what I need to do. First I need to append the user_input into the corpora and then I need to vectorize it.\n",
    "#after that I need to use cosine similarity on [-1] and the rest of the list\n",
    "#use argsort to select the [0][-2] from the array. this selects the one before the highest matching array\n",
    "#use flatten on the array. then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "60b3b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c76d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textresponse(text):\n",
    "    f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c94b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38240768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ed5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3f7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f4560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e970bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b53e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_cos(alpha):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69e85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d6b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07118429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080fc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b1274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1a13d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'manchester', 'city', '.', '\\n', 'I', 'hate', 'manchester', 'united', '.', '\\n', 'I', \"'ve\", 'be', 'watch', 'football', 'since', 'I', 'be', 'two', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# keeping only tagger component needed for lemmatization\n",
    "lemmatized_words = [token.lemma_ for token in nlp(my_str)]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "93343e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i like manchester city.\\ni hate manchester united.\\ni've been watching football since i was two.\\n\""
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "a64873e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [320]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# keeping only tagger component needed for lemmatization\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_words)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\language.py:1014\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    995\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    998\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    999\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1016\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\language.py:1108\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n\u001b[1;32m-> 1108\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1041\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[1;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>"
     ]
    }
   ],
   "source": [
    "# keeping only tagger component needed for lemmatization\n",
    "lemmatized_words = [token.lemma_ for token in nlp(ts)]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7909f856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639a7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a239ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8119cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = 'Python is the greatest language in the world'\n",
    "def \n",
    "doc = nlp(my_str)\n",
    "words_lemmas_list = [token.lemma_ for token in doc]\n",
    "print(words_lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73256710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizingword(text):\n",
    "    doc = nlp(text)\n",
    "    words_lemmas_list = [token.lemma_ for token in doc]\n",
    "    return(words_lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd786819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e296657",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f76a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab747cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemTokens(tokens):\n",
    "    return [ lemmatizingword(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ada467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "000d301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'be', 'the', 'great', 'language', 'in', 'the', 'world']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05196e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacypreprocessing(ik):\n",
    "    value = nlp(ik)\n",
    "    return [token.lemma_ for token in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c0ad49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'my', 'name', 'is']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sent_tokens = nltk.sent_tokenize(responses)\n",
    "#word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f680007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatresponse(text):\n",
    "    lower_response = [x.lower() for x in responses]\n",
    "    word_tokenization(text)\n",
    "    tvec = TfidfVectorizer(tokenizer = token.lemma_, stop_words='english')\n",
    "    tfidf = tvec.fit_transform(word_tokenization)\n",
    "    vals = cosine_similarity(tfidf[-1],tvec)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f6cca5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacypreprocessing(ik):\n",
    "    value = nlp(ik)\n",
    "    return [token.lemma_ for token in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5579b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6835bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e595e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=open('responses.txt','r',errors = 'ignore')\n",
    "resp1 =k1.read()\n",
    "resp1 = resp1.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's what I need to do. First I need to append the user_input into the corpora and then I need to vectorize it.\n",
    "#after that I need to use cosine similarity on [-1] and the rest of the list\n",
    "#use argsort to select the [0][-2] from the array. this selects the one before the highest matching array\n",
    "#use flatten on the array. then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a93079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1a22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe910c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4db715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
