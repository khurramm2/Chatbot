{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbedf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mustafa\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Warming up PyWSD (takes ~10 secs)... took 4.441178560256958 secs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from random import randrange\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('all')\n",
    "import wordnet\n",
    "nlp = spacy.load('en_core_web_lg',  disable=[\"parser\", \"ner\"])\n",
    "import io\n",
    "import random\n",
    "import string \n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag\n",
    "wnl = WordNetLemmatizer()\n",
    "warnings.filterwarnings('ignore')\n",
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf83bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "    \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "#print (type(response.json()))\n",
    "#print(response.json())\n",
    "df = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "playerid = 0\n",
    "goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "playercode = {'E. Haaland':0, 'H. Kane':1, 'A. Mitrović':2, 'I. Toney':3, 'L. Trossard':4, 'M. Almirón':5, 'P. Foden':6, 'Roberto Firmino':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b66aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askplayer(name):\n",
    "    askq = input(\"I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "    if askq =='yes' or askq == 'Yes':\n",
    "        print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "        print(\"E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\")  \n",
    "       #playerchoose = input(\"So what's your choice \")\n",
    "        while True:\n",
    "            playerchoose = input(\"So what's your choice \")\n",
    "            if playerchoose in playercode.keys():\n",
    "                playerid = (playercode[playerchoose])\n",
    "                #print(playerid)\n",
    "                print(\"hmm...\")\n",
    "                time.sleep(1)\n",
    "                print(\"Ah yes... \" + playerchoose + \" I know all about him. Did you know in this season he has scored a total of \" \n",
    "                      + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                return(True)\n",
    "                exit()\n",
    "\n",
    "            else:\n",
    "                print(\"That's not one of the names I gave you... Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b467f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a = \"I've been watching football since I was two\"\n",
    "response_b = \"I hate Manchester City\"\n",
    "response_c = \"hello I like wednesdays. John anything today\"\n",
    "response_d = \"my favorite team is Manchester United\"\n",
    "response_e = \"I've been watching football since I was two\"\n",
    "blank_spot = \"player\"\n",
    "\n",
    "\n",
    "responses = [response_a, response_b, response_c, response_d, response_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e719221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \n"
     ]
    }
   ],
   "source": [
    "askplayer(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5093c29",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "180fd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=open('responses.txt','r',errors = 'ignore')\n",
    "resp1 =k1.read()\n",
    "resp1 = resp1.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba1b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't know if I need this\n",
    "def lemmatization(text):\n",
    "    #lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "    for word in lemmatized_words:\n",
    "        if word in stopwords.words('english'):\n",
    "            lemmatized_words.remove(word)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e848ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in entity_list:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33bd5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my name is'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"hello my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea43d66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_respone = word_tokenization(resp1)\n",
    "tagged_response = pos_tag(tokenized_respone)\n",
    "lemaitized_response = lemmatization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96377fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "  #  for word in text:\n",
    "   #     if 'NN' in word[1]:\n",
    "    #        entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in text:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9434d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c226232",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f48a36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "lmt_corp =  nltk.sent_tokenize(lemmatization(resp1))\n",
    "#may have to reset sent_token every time\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756bebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc642a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like manchester city.',\n",
       " 'i hate manchester united.',\n",
       " \"i've been watching football since i was two.\",\n",
       " 'catci cars cat running dogs']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24a6b5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like manchester city\\ni hate manchester united\\nive been watching football since i was two\\ncatci cars cat running dogs\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "385fb58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like manchester city\\ni hate manchester united\\nive been watching football since i was two\\ncatci cars cat running dogs']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6202425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemTokens(tokens):\n",
    "    return [wnl.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade3f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "#may have to reset sent_token every time\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    mis_undlist = [\"I am sorry! I don't understand you\",\"I didn't quite get that\",\"I'm sorry can you repeat that\",\"I misunderstood that\",\"I'm sorry what\"]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+mis_undlist[randrange(4)]\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        print( robo_response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e792d25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I misunderstood that'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textresponse(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abc17b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    sent_tokens = nltk.sent_tokenize(resp1)\n",
    "    word_tokens = nltk.word_tokenize(resp1)\n",
    "    lmt_string = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for word in exit_command:\n",
    "        if word in userinput:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "            exit()\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in userinput:\n",
    "        askplayer(\"example\")\n",
    "        \n",
    "    \n",
    "    userinput2 = input(listintro[randrange(4)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07e420cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Garry, I will be introducing you to the world of football \n",
      "What else\n"
     ]
    }
   ],
   "source": [
    "speech(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e6929a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Garry, I will be introducing you to the world of football \n"
     ]
    }
   ],
   "source": [
    "exit_command = ['bye','exit,','goodbye']\n",
    "\n",
    "userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "for word in userinput:\n",
    "    if word in exit_command:\n",
    "        print(\"goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2e24ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    frequency = 0\n",
    "    for word in user_input:\n",
    "        if word.lower() in responses_option:\n",
    "            frequency +=1\n",
    "    return frequency\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9029e7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m frequency_of_terms(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello my name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43melist\u001b[49m[\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'elist' is not defined"
     ]
    }
   ],
   "source": [
    "frequency_of_terms(\"hello my name\",elist[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "124c7dd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frequency_of_terms2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfrequency_of_terms2\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m,elist[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frequency_of_terms2' is not defined"
     ]
    }
   ],
   "source": [
    "frequency_of_terms2(\"city\",elist[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf9ede4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i like manchester city.\\ni hate manchester united.\\ni've been watching football since i was two.\\ncatci cars cat running dogs\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc18a5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like manchester city.',\n",
       " 'i hate manchester united.',\n",
       " \"i've been watching football since i was two.\",\n",
       " 'catci cars cat running dogs',\n",
       " 'hello']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfb8084d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3733349992.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [33]\u001b[1;36m\u001b[0m\n\u001b[1;33m    if responses_option[:]\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    frequency = 0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in responses_option[n] :\n",
    "            if word in item:\n",
    "                if responses_option[:]\n",
    "    print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "035e964c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_of_terms(\"cat and dogs\",sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc3e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9f9c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursion(text):\n",
    " #  text = int(text)\n",
    "    if text<6:\n",
    "        print(\"your number is \" + str(text))\n",
    "        recursion(text+1)\n",
    "    else:\n",
    "        print(\"noo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1598f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,response_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in response_option:\n",
    "            if word in item:\n",
    "                frequency +=1\n",
    "                #if response_option[n] <= response_option[3]:\n",
    "                 #   n=n+1\n",
    "            freqhash.append(frequency)\n",
    "    print(frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ba905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a23c5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    n=0\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    user_input = user_input.split()\n",
    "    #while responses_option[n]\n",
    "    for word in user_input:\n",
    "        if word in responses_option[n]:\n",
    "            frequency +=1\n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a67a33b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "frequency_of_terms(\"cat and dogs\",sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6a60468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24adbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = np.array(freqhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e63d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,response_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in response_option[3]:\n",
    "            if word in item:\n",
    "                frequency +=1\n",
    "                #if response_option[n] <= response_option[3]:\n",
    "                 #   n=n+1\n",
    "            freqhash.append(frequency)\n",
    "    print(frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf82a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc707e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option,n):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    \n",
    " #   user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        if word in responses_option:\n",
    "            frequency +=1\n",
    "            \n",
    "   # if n<3\n",
    "    frequency_of_terms(user_input,responses_option,(1))\n",
    "            \n",
    "            \n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b435486",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_terms(\"cat and dog\",sent_tokens,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff96b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d106a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bce22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b96e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1dd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5480916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        if word in responses_option:\n",
    "            frequency +=1\n",
    "            n=n+1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda5e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5b574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab22f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958bca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "163bc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in responses_option:\n",
    "            if word in item[n]:\n",
    "                frequency +=1\n",
    "                n=n+1\n",
    "    \n",
    "    #while n<3:\n",
    "     #   frequency_of_terms(user_input,responses_option)\n",
    "        \n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "3b4ac815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in responses_option:\n",
    "            if word in item[n]:\n",
    "                frequency +=1\n",
    "                n=n+1\n",
    "    \n",
    "   # while n<3:\n",
    "    #    frequency_of_terms(user_input,responses_option)\n",
    "        \n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "015f58b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "frequency_of_terms(\"cat and dog\",sent_tokens[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccedfc00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msent_tokens\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent_tokens:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(word)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "sent_tokens\n",
    "for word in sent_tokens:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "9fc4735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = [\"num\",\"da\",\"yas\",\"okoko\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 0\n",
    "while l2[n] < l2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c11099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,response_option):\n",
    "    global freqhash\n",
    "    frequency = 0\n",
    "    freqhash = []\n",
    "    n=0\n",
    "    user_input = user_input.split()\n",
    "    for word in user_input:\n",
    "        for item in response_option:\n",
    "            if response_option[n] <response_option[3]:\n",
    "                frequency +=1\n",
    "                if response_option[n] <= response_option[-1]:\n",
    "                    n=n+1\n",
    "    print(frequency)\n",
    "    freqhash.append(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad054f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "20034714",
   "metadata": {},
   "outputs": [],
   "source": [
    "elist = []\n",
    "for word in sent_tokens:\n",
    "    elist.append(lemmatize_sentence(word,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "8dcd4496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'like', 'manchester', 'city', '.'],\n",
       " ['i', 'hate', 'manchester', 'unite', '.'],\n",
       " ['i', \"'ve\", 'be', 'watch', 'football', 'since', 'i', 'be', 'two', '.'],\n",
       " ['catci', 'car', 'cat', 'run', 'dog']]"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "faae90a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "hate\n",
      "'ve\n",
      "car\n"
     ]
    }
   ],
   "source": [
    "for item in elist:\n",
    "    print(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d7231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d4db715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed24ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    #lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "    for word in lemmatized_words:\n",
    "        if word in stopwords.words('english'):\n",
    "            lemmatized_words.remove(word)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfinished\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    tokenized_respone = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_respone)\n",
    "    taggandlem(tagged_response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9ea232a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "\n",
    "    for lit in entity_list:\n",
    "        print(wnl.lemmatize(lit))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbe175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization2(entity):\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(entity)]\n",
    "    return(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''=\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    mis_undlist = [\"I am sorry! I don't understand you\",\"I didn't quite get that\",\"I'm sorry can you repeat that\",\"I misunderstood that\",\"I'm sorry what\"]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+mis_undlist[randrange(4)]\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253da443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    \n",
    "    for word in exit_command:\n",
    "        if word in userinput:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in userinput:\n",
    "        askplayer(\"example\")\n",
    "        \n",
    "    \n",
    "    userinput2 = input(listintro[randrange(4)])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
