{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "8bbedf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from random import randrange\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('all')\n",
    "import wordnet\n",
    "nlp = spacy.load('en_core_web_lg',  disable=[\"parser\", \"ner\"])\n",
    "import io\n",
    "import random\n",
    "import string \n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag\n",
    "wnl = WordNetLemmatizer()\n",
    "warnings.filterwarnings('ignore')\n",
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf83bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "    \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "#print (type(response.json()))\n",
    "#print(response.json())\n",
    "df = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da3ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "playerid = 0\n",
    "goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "playercode = {'E. Haaland':0, 'H. Kane':1, 'A. Mitrović':2, 'I. Toney':3, 'L. Trossard':4, 'M. Almirón':5, 'P. Foden':6, 'Roberto Firmino':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0b66aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askplayer(name):\n",
    "    askq = input(\"I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "    if askq =='yes' or askq == 'Yes':\n",
    "        print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "        print(\"E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\")  \n",
    "       #playerchoose = input(\"So what's your choice \")\n",
    "        while True:\n",
    "            playerchoose = input(\"So what's your choice \")\n",
    "            if playerchoose in playercode.keys():\n",
    "                playerid = (playercode[playerchoose])\n",
    "                #print(playerid)\n",
    "                print(\"hmm...\")\n",
    "                time.sleep(1)\n",
    "                print(\"Ah yes... \" + playerchoose + \" I know all about him. Did you know in this season he has scored a total of \" \n",
    "                      + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                return(True)\n",
    "                exit()\n",
    "\n",
    "            else:\n",
    "                print(\"That's not one of the names I gave you... Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "40b467f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a = \"I've been watching football since I was two\"\n",
    "response_b = \"I hate Manchester City\"\n",
    "response_c = \"hello I like wednesdays. John anything today\"\n",
    "response_d = \"my favorite team is Manchester United\"\n",
    "response_e = \"I've been watching football since I was two\"\n",
    "blank_spot = \"player\"\n",
    "\n",
    "\n",
    "responses = [response_a, response_b, response_c, response_d, response_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "790c3152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \n"
     ]
    }
   ],
   "source": [
    "askplayer(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e5093c29",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "180fd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=open('responses.txt','r',errors = 'ignore')\n",
    "resp1 =k1.read()\n",
    "resp1 = resp1.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5a038f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't know if I need this\n",
    "def lemmatization(text):\n",
    "    #lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "    for word in lemmatized_words:\n",
    "        if word in stopwords.words('english'):\n",
    "            lemmatized_words.remove(word)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "41cc6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in entity_list:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60763b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3df63bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my name is'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"hello my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2ea43d66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_respone = word_tokenization(resp1)\n",
    "tagged_response = pos_tag(tokenized_respone)\n",
    "lemaitized_response = lemmatization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "96377fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "  #  for word in text:\n",
    "   #     if 'NN' in word[1]:\n",
    "    #        entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in text:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b93cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "704a90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "df30ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "lmt_corp =  nltk.sent_tokenize(lemmatization(resp1))\n",
    "#may have to reset sent_token every time\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66539d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c67da6bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like manchester city.',\n",
       " 'i hate manchester united.',\n",
       " \"i've been watching football since i was two.\"]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9454796f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like manchester city\\ni hate manchester united\\nive been watching football since i was two\\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b132dd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like manchester city\\ni hate manchester united\\nive been watching football since i was two']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6202425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemTokens(tokens):\n",
    "    return [wnl.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ade3f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "#may have to reset sent_token every time\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    mis_undlist = [\"I am sorry! I don't understand you\",\"I didn't quite get that\",\"I'm sorry can you repeat that\",\"I misunderstood that\",\"I'm sorry what\"]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+mis_undlist[randrange(4)]\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        print( robo_response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e82d346b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am sorry! I don't understand you\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textresponse(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db549f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    sent_tokens = nltk.sent_tokenize(resp1)\n",
    "    word_tokens = nltk.word_tokenize(resp1)\n",
    "    lmt_string = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for word in exit_command:\n",
    "        if word in userinput:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "            exit()\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in userinput:\n",
    "        askplayer(\"example\")\n",
    "        \n",
    "    \n",
    "    userinput2 = input(listintro[randrange(4)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9fd91909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Garry, I will be introducing you to the world of football bye\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You have chosen to end the chat'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e103ff2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Garry, I will be introducing you to the world of football bye\n"
     ]
    }
   ],
   "source": [
    "exit_command = ['bye','exit,','goodbye']\n",
    "\n",
    "userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "for word in userinput:\n",
    "    if word in exit_command:\n",
    "        print(\"goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3942a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms(user_input,responses_option):\n",
    "    frequency = 0\n",
    "    for word in user_input:\n",
    "        if word.lower() in responses_option:\n",
    "            frequency +=1\n",
    "    return frequency\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "89ef8725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_of_terms(\"hello\",\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9025908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_of_terms2(\"city\",sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "cb00d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i like manchester city.\\ni hate manchester united.\\ni've been watching football since i was two.\\ncatci cars cat running dogs\\n\""
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7f189292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like manchester city.',\n",
       " 'i hate manchester united.',\n",
       " \"i've been watching football since i was two.\",\n",
       " 'catci cars cat running dogs']"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "e91475e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_terms2(user_input,responses_option):\n",
    "    frequency = 0\n",
    "    jlist = []\n",
    "    for word in user_input:\n",
    "        for item in responses_option:\n",
    "            if word in item:\n",
    "                frequency +=1\n",
    "                jlist.append(frequency)\n",
    "                frequncy = 0\n",
    "    return frequency\n",
    "    print(jlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "bf0b73af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_of_terms2(\"cat and dog\",sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "3835d2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_of_terms(\"hello\",\"hello my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "28cd4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "elist = []\n",
    "for word in sent_tokens:\n",
    "    elist.append(lemmatize_sentence(word,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "5235f1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'like', 'manchester', 'city', '.'],\n",
       " ['i', 'hate', 'manchester', 'unite', '.'],\n",
       " ['i', \"'ve\", 'be', 'watch', 'football', 'since', 'i', 'be', 'two', '.'],\n",
       " ['catci', 'car', 'cat', 'run', 'dog']]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "2b9f6d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'manchester', 'city', '.']\n",
      "['i', 'hate', 'manchester', 'unite', '.']\n",
      "['i', \"'ve\", 'be', 'watch', 'football', 'since', 'i', 'be', 'two', '.']\n",
      "['catci', 'car', 'cat', 'run', 'dog']\n"
     ]
    }
   ],
   "source": [
    "for word in elist:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 0\n",
    "numlist = [2,4,3,5,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "1b38c322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fb135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2999f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538281b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d4db715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed24ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    #lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "    for word in lemmatized_words:\n",
    "        if word in stopwords.words('english'):\n",
    "            lemmatized_words.remove(word)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfinished\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    tokenized_respone = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_respone)\n",
    "    taggandlem(tagged_response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9ea232a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "\n",
    "    for lit in entity_list:\n",
    "        print(wnl.lemmatize(lit))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbe175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization2(entity):\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(entity)]\n",
    "    return(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "lmt_string = []\n",
    "\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''=\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    mis_undlist = [\"I am sorry! I don't understand you\",\"I didn't quite get that\",\"I'm sorry can you repeat that\",\"I misunderstood that\",\"I'm sorry what\"]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+mis_undlist[randrange(4)]\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301da394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    userinput = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    \n",
    "    for word in exit_command:\n",
    "        if word in userinput:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in userinput:\n",
    "        askplayer(\"example\")\n",
    "        \n",
    "    \n",
    "    userinput2 = input(listintro[randrange(4)])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
