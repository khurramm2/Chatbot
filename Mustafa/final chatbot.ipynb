{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8bbedf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from random import randrange\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('all')\n",
    "import wordnet\n",
    "nlp = spacy.load('en_core_web_lg',  disable=[\"parser\", \"ner\"])\n",
    "import io\n",
    "import random\n",
    "import string \n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag\n",
    "wnl = WordNetLemmatizer()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf83bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "    \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "#print (type(response.json()))\n",
    "#print(response.json())\n",
    "df = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "playerid = 0\n",
    "goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "playercode = {'E. Haaland':0, 'H. Kane':1, 'A. Mitrović':2, 'I. Toney':3, 'L. Trossard':4, 'M. Almirón':5, 'P. Foden':6, 'Roberto Firmino':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea7939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b66aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askplayer(name):\n",
    "    askq = input(\"I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "    if askq =='yes' or askq == 'Yes':\n",
    "        print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "        print(\"E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\")  \n",
    "       #playerchoose = input(\"So what's your choice \")\n",
    "        while True:\n",
    "            playerchoose = input(\"So what's your choice \")\n",
    "            if playerchoose in playercode.keys():\n",
    "                playerid = (playercode[playerchoose])\n",
    "                #print(playerid)\n",
    "                print(\"hmm...\")\n",
    "                time.sleep(1)\n",
    "                print(\"Ah yes... \" + playerchoose + \" I know all about him. Did you know in this season he has scored a total of \" \n",
    "                      + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                break\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"That's not one of the names I gave you... Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d9de705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? yes\n",
      "Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\n",
      "E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\n",
      "So what's your choice E. Haaland\n",
      "hmm...\n",
      "Ah yes... E. Haaland I know all about him. Did you know in this season he has scored a total of 18 goals and he's attempted a grande total of 43 shots\n"
     ]
    }
   ],
   "source": [
    "askplayer(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c74ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec = spacy.load(\"en_core_web_sm\")\n",
    "# if on jupyter notebook open up the anaconda terminal and typeb-m spacy download en_core_web_lg, if it isn't installed\n",
    "#replace spacy.load(\"en\") with spacy.load(\"en_core_web_sm\")\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt')\n",
    "#python -m spacy download en   ...do this in the conda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b467f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a = \"I've been watching football since I was two\"\n",
    "response_b = \"I hate Manchester City\"\n",
    "response_c = \"hello I like wednesdays. John anything today\"\n",
    "response_d = \"my favorite team is Manchester United\"\n",
    "response_e = \"I've been watching football since I was two\"\n",
    "blank_spot = \"player\"\n",
    "\n",
    "\n",
    "responses = [response_a, response_b, response_c, response_d, response_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "556bccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordIn(text):\n",
    "    hjk = list()\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            hjk.append(word[0])\n",
    "    return(hjk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a49e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech(text):\n",
    "    qans = input(\"Hello my name is Garry, I will be introducing you to the world of football \")\n",
    "    exit_command = ['bye','exit,','goodbye']\n",
    "    listintro = [\"Do you have anything else that you want to talk about? \",\"What else do you want to ask?\",\"Is there anything else?\",\"What else\"]\n",
    "    qans2 = input(listintro[randrange(4)])\n",
    "    for word in exit_command:\n",
    "        if word in qans:\n",
    "            return(\"You have chosen to end the chat\")\n",
    "    \n",
    "    \n",
    "    if \"top scorer\" in qans:\n",
    "        askplayer(\"example\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5093c29",
   "metadata": {},
   "outputs": [],
   "source": [
    " def word_tokenization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stopwords.words('english'):\n",
    "            text_tokenized.remove(word)\n",
    "        \n",
    "    return(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "180fd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=open('responses.txt','r',errors = 'ignore')\n",
    "resp1 =k1.read()\n",
    "resp1 = resp1.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2ea43d66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_respone = word_tokenization(resp1)\n",
    "tagged_response = pos_tag(tokenized_respone)\n",
    "lemaitized_response = lemmatization(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71b1168b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city', 'hate', 'manchester', 'watching', 'football']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list = []\n",
    "for word in tagged_response:\n",
    "    if 'NN' in word[1]:\n",
    "        entity_list.append(word[0])\n",
    "        \n",
    "entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd050456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "hate\n",
      "manchester\n",
      "watching\n",
      "football\n"
     ]
    }
   ],
   "source": [
    "for word in entity_list:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "96377fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    global lemmatized_list \n",
    "    global lmt_string\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "    \n",
    "    lemmatized_list = []\n",
    "    for lit in entity_list:\n",
    "        lmtz = wnl.lemmatize(lit)\n",
    "        lemmatized_list.append(lmtz)\n",
    "        lmt_string = ' '.join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c12f4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "taggandlem(tagged_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bf888c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's what I need to do. First I need to append the user_input into the corpora and then I need to vectorize it.\n",
    "#after that I need to use cosine similarity on [-1] and the rest of the list\n",
    "#use argsort to select the [0][-2] from the array. this selects the one before the highest matching array\n",
    "#use flatten on the array. then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f6202425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemTokens(tokens):\n",
    "    return [wnl.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "02bfb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ade3f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(resp1)\n",
    "word_tokens = nltk.word_tokenize(resp1)\n",
    "\n",
    "#may have to reset sent_token every time\n",
    "\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    tokenized_response = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_response)\n",
    "    taggandlem(tagged_response)\n",
    "    sent_tokens.append(lmt_string)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    mis_undlist = [\"I am sorry! I don't understand you\",\"I didn't quite get that\",\"I'm sorry can you repeat that\",\"I misunderstood that\",\"I'm sorry what\"]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+mis_undlist[randrange(4)]\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5423cdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am sorry! I don't understand you\""
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textresponse(\"asdouiifis bafd dasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6262c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db3395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70d3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f018674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a069b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed24ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d4db715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    #lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "    for word in lemmatized_words:\n",
    "        if word in stopwords.words('english'):\n",
    "            lemmatized_words.remove(word)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61670b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfinished\n",
    "def textresponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    tokenized_respone = word_tokenization(user_response)\n",
    "    tagged_response = pos_tag(tokenized_respone)\n",
    "    taggandlem(tagged_response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9ea232a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taggandlem(text):\n",
    "    entity_list = []\n",
    "    for word in text:\n",
    "        if 'NN' in word[1]:\n",
    "            entity_list.append(word[0])\n",
    "\n",
    "    for lit in entity_list:\n",
    "        print(wnl.lemmatize(lit))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbe175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization2(entity):\n",
    "    lemmatized_words = [token.lemma_ for token in nlp(entity)]\n",
    "    return(lemmatized_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
