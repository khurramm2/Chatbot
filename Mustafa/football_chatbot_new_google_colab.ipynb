{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed840d25",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/khurramm2/Chatbot/blob/main/football_chatbot_new_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfbc6ec",
   "metadata": {
    "id": "dcfbc6ec"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "word2vec = spacy.load(\"en_core_web_sm\")\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8da1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39d8da1e",
    "outputId": "8187e4d3-d761-4c0a-d678-bc1ca47df0d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace spacy.load(\"en\") with spacy.load(\"en_core_web_sm\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "#python -m spacy download en   ...do this in the conda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EczZxt_980_h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EczZxt_980_h",
    "outputId": "52ff5063-9d84-4c1a-ae9f-c3dedd65d99e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zbaXKYOM9Xji",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbaXKYOM9Xji",
    "outputId": "503f27db-536e-4d0d-9396-05e973ecae21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e82ec",
   "metadata": {
    "id": "3f6e82ec"
   },
   "outputs": [],
   "source": [
    "\n",
    "    import requests\n",
    "\n",
    "    url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "    querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "        \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "    #print (type(response.json()))\n",
    "    #print(response.json())\n",
    "    df = response.json()\n",
    "    \n",
    "    playerid = 0\n",
    "    goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "    shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "    passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "    playercode = {'E. Haaland':0, 'H. Kane':1, 'A. Mitrović':2, 'I. Toney':3, 'L. Trossard':4, 'M. Almirón':5, 'P. Foden':6, 'Roberto Firmino':7}\n",
    "\n",
    "    def askplayer(name):\n",
    "        askq = input(\"I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "        if askq =='yes' or askq == 'Yes':\n",
    "            print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "            print(\"E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\")  \n",
    "           #playerchoose = input(\"So what's your choice \")\n",
    "            while True:\n",
    "                playerchoose = input(\"So what's your choice \")\n",
    "                if playerchoose in playercode.keys():\n",
    "                    playerid = (playercode[playerchoose])\n",
    "                    #print(playerid)\n",
    "                    print(\"Ah yes... \" + playerchoose + \"I know all about him. Did you know in this season he has scored a total of \" \n",
    "                          + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"That's not one of the names I gave you... Try again.\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed2bce",
   "metadata": {
    "id": "8aed2bce",
    "outputId": "64babf1b-48f3-442a-973b-3684a52583a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helloNone'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#askplayer(\"s\")\n",
    "#askp = askplayer(\"s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5ba5c",
   "metadata": {
    "id": "d8a5ba5c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_a = \"I am an expert at football\"\n",
    "response_b = \"I hate Manchester City\"\n",
    "response_c = \"yeah I like {}\"\n",
    "response_d = \"my favorite team is Manchester United\"\n",
    "response_e = \"I've been watching football since I was two\"\n",
    "blank_spot = \"player\"\n",
    "\n",
    "\n",
    "responses = [response_a, response_b, response_c, response_d, response_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd0c6b",
   "metadata": {
    "id": "e5cd0c6b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8c424",
   "metadata": {
    "id": "75e8c424"
   },
   "outputs": [],
   "source": [
    "exit_commands = (\"quit\", \"goodbye\", \"exit\", \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c987b",
   "metadata": {
    "id": "0e6c987b"
   },
   "outputs": [],
   "source": [
    "def preprocess(input_sentence):\n",
    "    input_sentence = input_sentence.lower()\n",
    "    input_sentence = re.sub(r'[^\\w\\s]','',input_sentence)\n",
    "    tokens = word_tokenize(input_sentence)\n",
    "    input_sentence = [i for i in tokens if not i in stop_words]\n",
    "    return(input_sentence)\n",
    "\n",
    "def compare_overlap(user_message, possible_response):\n",
    "    similar_words = 0\n",
    "    for token in user_message:\n",
    "        if token in possible_response:\n",
    "              similar_words += 1\n",
    "    return similar_words\n",
    "  \n",
    "def extract_nouns(tagged_message):\n",
    "    message_nouns = list()\n",
    "    for token in tagged_message:\n",
    "        if token[1].startswith(\"N\"):\n",
    "            message_nouns.append(token[0])\n",
    "    return message_nouns\n",
    "\n",
    "def compute_similarity(tokens, category):\n",
    "    output_list = list()\n",
    "    for token in tokens:\n",
    "        output_list.append([token.text, category.text, token.similarity(category)])\n",
    "    return output_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6e8c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16d6e8c6",
    "outputId": "979e8507-5ce2-4bef-90bf-d815b23e2b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! I know a lot about football is there anything you want to know? whats your favourite team bitch?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my favorite team is Manchester United\n",
      "Do you have any other questions? what team do yo hate\n",
      "I hate Manchester City\n",
      "Do you have any other questions? do you like David\n",
      "yeah I like david\n",
      "Do you have any other questions? Do you like your mother\n",
      "yeah I like mother\n",
      "Do you have any other questions? cock\n",
      "I am an expert at football\n",
      "Do you have any other questions? do you like cock\n",
      "yeah I like cock\n",
      "Do you have any other questions? do you like pussy\n",
      "yeah I like pussy\n",
      "Do you have any other questions? exit\n",
      "Ok, bye!\n"
     ]
    }
   ],
   "source": [
    "class ChatBot:\n",
    "\n",
    "    def make_exit(self, user_message):\n",
    "        for exit_command in exit_commands:\n",
    "            if exit_command in user_message:\n",
    "                print(\"Ok, bye!\")\n",
    "                return True\n",
    "        \n",
    "    def chat(self):\n",
    "        user_message = input(\"Hey! I know a lot about football is there anything you want to know? \")\n",
    "        while not self.make_exit(user_message):\n",
    "            user_message = self.respond(user_message)\n",
    "        \n",
    "\n",
    "    \n",
    "    def find_intent_match(self, responses, user_message):\n",
    "        bow_user_message = Counter(preprocess(user_message))\n",
    "        processed_responses = [Counter(preprocess(response)) for response in responses]\n",
    "        similarity_list = [compare_overlap(doc, bow_user_message) for doc in processed_responses]\n",
    "        response_index = similarity_list.index(max(similarity_list))\n",
    "        return responses[response_index]\n",
    "\n",
    "\n",
    "    def respond(self, user_message):\n",
    "        best_response = self.find_intent_match(responses, user_message)\n",
    "        entity = self.find_entities(user_message)\n",
    "        print(best_response.format(entity))\n",
    "        input_message = input(\"Do you have any other questions? \")           \n",
    "        return input_message\n",
    "        \n",
    "    def find_entities(self, user_message):\n",
    "          # extract candidate entities\n",
    "        tagged_user_message = pos_tag(preprocess(user_message))\n",
    "        message_nouns = extract_nouns(tagged_user_message)\n",
    "        tokens = word2vec(\" \".join(message_nouns))\n",
    "        category = word2vec(blank_spot)\n",
    "        word2vec_result = compute_similarity(tokens, category)\n",
    "        word2vec_result.sort(key=lambda x: x[2])\n",
    "        if len(word2vec_result) < 1:\n",
    "            return blank_spot\n",
    "        else:\n",
    "            return word2vec_result[-1][0]\n",
    "\n",
    "\n",
    "cantina_bot = ChatBot()\n",
    "cantina_bot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e463d",
   "metadata": {
    "id": "9f4e463d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
