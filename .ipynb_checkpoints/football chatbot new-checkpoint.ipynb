{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfbc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "word2vec = spacy.load(\"en_core_web_sm\")\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d8da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace spacy.load(\"en\") with spacy.load(\"en_core_web_sm\")\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt')\n",
    "#python -m spacy download en   ...do this in the conda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6e82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    import requests\n",
    "\n",
    "    url = \"https://api-football-beta.p.rapidapi.com/players/topscorers\"\n",
    "\n",
    "    querystring = {\"season\":\"2022\",\"league\":\"39\"}\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"62a02ca5ccmsha68a5acf6c698d9p1ffc12jsnbe2b9a765139\",\n",
    "        \"X-RapidAPI-Host\": \"api-football-beta.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "    #print (type(response.json()))\n",
    "    #print(response.json())\n",
    "    df = response.json()\n",
    "    \n",
    "    playerid = 0\n",
    "    goals = df['response'][playerid]['statistics'][0]['goals']['total']\n",
    "    shot = df['response'][playerid]['statistics'][0]['shots']['total']\n",
    "    passing = df['response'][playerid]['statistics'][0]['passes']['total']\n",
    "    playercode = {'E. Haaland':0, 'H. Kane':1, 'A. Mitrović':2, 'I. Toney':3, 'L. Trossard':4, 'M. Almirón':5, 'P. Foden':6, 'Roberto Firmino':7}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aed2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#askplayer(\"s\")\n",
    "#askp = askplayer(\"s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a5ba5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_a = \"I am an expert at football\"\n",
    "response_b = \"I hate Manchester City\"\n",
    "response_c = \"yeah I like {}\"\n",
    "response_d = \"my favorite team is Manchester United\"\n",
    "response_e = \"I've been watching football since I was two\"\n",
    "blank_spot = \"player\"\n",
    "\n",
    "\n",
    "responses = [response_a, response_b, response_c, response_d, response_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd0c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e8c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_commands = (\"quit\", \"goodbye\", \"exit\", \"no\",\"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6c987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_sentence):\n",
    "    input_sentence = input_sentence.lower()\n",
    "    input_sentence = re.sub(r'[^\\w\\s]','',input_sentence)\n",
    "    tokens = word_tokenize(input_sentence)\n",
    "    input_sentence = [i for i in tokens if not i in stop_words]\n",
    "    return(input_sentence)\n",
    "\n",
    "def compare_overlap(user_message, possible_response):\n",
    "    similar_words = 0\n",
    "    for token in user_message:\n",
    "        if token in possible_response:\n",
    "              similar_words += 1\n",
    "    return similar_words\n",
    "  \n",
    "def extract_nouns(tagged_message):\n",
    "    message_nouns = list()\n",
    "    for token in tagged_message:\n",
    "        if token[1].startswith(\"N\"):\n",
    "            message_nouns.append(token[0])\n",
    "    return message_nouns\n",
    "\n",
    "def compute_similarity(tokens, category):\n",
    "    output_list = list()\n",
    "    for token in tokens:\n",
    "        output_list.append([token.text, category.text, token.similarity(category)])\n",
    "    return output_list\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d6e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! I know a lot about football is there anything you want to know? what is your favourite team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_10856\\304742894.py:25: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  output_list.append([token.text, category.text, token.similarity(category)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my favorite team is Manchester United\n",
      "Do you have any other questions? what team do you hate\n",
      "I hate Manchester City\n",
      "Do you have any other questions? no\n",
      "Ok, bye!\n"
     ]
    }
   ],
   "source": [
    "class ChatBot:\n",
    "\n",
    "    def make_exit(self, user_message):\n",
    "        for exit_command in exit_commands:\n",
    "            if exit_command in user_message:\n",
    "                print(\"Ok, bye!\")\n",
    "                return True\n",
    "        \n",
    "    def chat(self):\n",
    "        user_message = input(\"Hey! I know a lot about football is there anything you want to know? \")\n",
    "        while not self.make_exit(user_message):\n",
    "            user_message = self.respond(user_message)\n",
    "        \n",
    "\n",
    "    \n",
    "    def find_intent_match(self, responses, user_message):\n",
    "        bow_user_message = Counter(preprocess(user_message))\n",
    "        processed_responses = [Counter(preprocess(response)) for response in responses]\n",
    "        similarity_list = [compare_overlap(doc, bow_user_message) for doc in processed_responses]\n",
    "        response_index = similarity_list.index(max(similarity_list))\n",
    "        return responses[response_index]\n",
    "\n",
    "\n",
    "    def respond(self, user_message):\n",
    "        best_response = self.find_intent_match(responses, user_message)\n",
    "        entity = self.find_entities(user_message)\n",
    "        print(best_response.format(entity))\n",
    "        input_message = input(\"Do you have any other questions? \")           \n",
    "        return input_message\n",
    "        \n",
    "    def find_entities(self, user_message):\n",
    "          # extract candidate entities\n",
    "        tagged_user_message = pos_tag(preprocess(user_message))\n",
    "        message_nouns = extract_nouns(tagged_user_message)\n",
    "        tokens = word2vec(\" \".join(message_nouns))\n",
    "        category = word2vec(blank_spot)\n",
    "        word2vec_result = compute_similarity(tokens, category)\n",
    "        word2vec_result.sort(key=lambda x: x[2])\n",
    "        if len(word2vec_result) < 1:\n",
    "            return blank_spot\n",
    "        else:\n",
    "            return word2vec_result[-1][0]\n",
    "\n",
    "\n",
    "    def askplayer(name):\n",
    "        askq = input(\"I know a lot about football, if you want I can tell you about some of the top scorers in the premiere league. So what do you say, Yes or No? \")\n",
    "        if askq =='yes' or askq == 'Yes':\n",
    "            print(\"Here are the names of some of the top scorers I know, Choose any of those names and I'll tell you all about them\")\n",
    "            print(\"E. Haaland, H. Kane, A. Mitrović, I. Toney, L. Trossard, M. Almirón, P. Foden, Roberto Firmino\")  \n",
    "           #playerchoose = input(\"So what's your choice \")\n",
    "            while True:\n",
    "                playerchoose = input(\"So what's your choice \")\n",
    "                if playerchoose in playercode.keys():\n",
    "                    playerid = (playercode[playerchoose])\n",
    "                    #print(playerid)\n",
    "                    print(\"Ah yes... \" + playerchoose + \"I know all about him. Did you know in this season he has scored a total of \" \n",
    "                          + str(goals) + \" goals and he's attempted a grande total of \" + str(shot) +\" shots\")\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    print(\"That's not one of the names I gave you... Try again.\")\n",
    "                    \n",
    "cantina_bot = ChatBot()\n",
    "cantina_bot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4e463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Select language (english or albanian): english\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'conversation_dataset/english_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10856\\3182664487.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;31m# Load dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conversation_dataset/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_dataset.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'conversation_dataset/english_dataset.json'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "def tokenize_stem(string_input):\n",
    "\n",
    "    words = nltk.word_tokenize(string_input)\n",
    "    stemmed_words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def load_data(data, lang):\n",
    "    try:\n",
    "        with open('data/'+lang+'_labels_list.json', 'r') as labels_file, open('data/'+lang+'_pattern_vocab.json', 'r') as pattern_file:\n",
    "            labels_dict = json.load(labels_file)\n",
    "            pattern_vocab = json.load(pattern_file)\n",
    "            print('has data')\n",
    "            return labels_dict, pattern_vocab\n",
    "    except:\n",
    "        labels_dict, pattern_vocab, input_x, input_y = generate_data(\n",
    "            data, lang)\n",
    "        return labels_dict, pattern_vocab\n",
    "\n",
    "\n",
    "def generate_data(data, lang):\n",
    "    print('does not have data')\n",
    "    # Separate patterns\n",
    "    questions = []\n",
    "    labels = []\n",
    "    labels_dict = []\n",
    "    for i, intents in enumerate(data['data']):\n",
    "        for pattern in intents['patterns']:\n",
    "            questions.append(pattern)\n",
    "            labels.append(intents['tag'])\n",
    "\n",
    "    # Vectorize patterns (Bag of words)\n",
    "    pattern_vec = CountVectorizer(binary=True, tokenizer=tokenize_stem)\n",
    "    pattern_matrix = pattern_vec.fit_transform(questions)\n",
    "    pattern_vocab = pattern_vec.vocabulary_\n",
    "    input_x = pattern_matrix.todense()\n",
    "    input_x = numpy.array(input_x)\n",
    "\n",
    "    # Vectorize labels (Bag of words)\n",
    "    labels_vec = CountVectorizer(\n",
    "        binary=True, lowercase=False, token_pattern='[a-zA-Z0-9$&+,:;=?@#|<>.^*()%!-]+')\n",
    "    labels_matrix = labels_vec.fit_transform(labels)\n",
    "    labels_vocab = labels_vec.vocabulary_\n",
    "    input_y = labels_matrix.todense()\n",
    "    input_y = numpy.array(input_y)\n",
    "\n",
    "    # Convert pattern_vocab values to integers and save it as a json file\n",
    "    for key in pattern_vocab:\n",
    "        pattern_vocab[key] = int(pattern_vocab[key])\n",
    "\n",
    "    with open('data/'+lang+'_pattern_vocab.json', 'w') as file:\n",
    "        json.dump(pattern_vocab, file)\n",
    "\n",
    "    # Separate labels_vocab keys, sort them and save it to a json file\n",
    "    for key in labels_vocab:\n",
    "        labels_dict.append(key)\n",
    "    labels_dict = sorted(labels_dict)\n",
    "    # print(labels_dict)\n",
    "\n",
    "    with open('data/'+lang+'_labels_list.json', 'w') as file:\n",
    "        json.dump(labels_dict, file)\n",
    "\n",
    "    return labels_dict, pattern_vocab, input_x, input_y\n",
    "\n",
    "\n",
    "def create_model(training, output):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(8, input_shape=(len(training[0]),)))\n",
    "    model.add(keras.layers.Dense(8))\n",
    "    model.add(keras.layers.Dense(len(output[0]), activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def chat(model, labels_dict, pattern_vocab):\n",
    "    print(\"You can chat with the bot now!\")\n",
    "    while True:\n",
    "        inp = input(\"Chat: \")\n",
    "\n",
    "        if str(inp) == '':\n",
    "            print(\"Bot: Say something!\")\n",
    "        else:\n",
    "            if inp.lower() == 'quit':\n",
    "                break\n",
    "\n",
    "            word_vec = CountVectorizer(\n",
    "                binary=True, vocabulary=pattern_vocab, tokenizer=tokenize_stem)\n",
    "            input_dict = [inp]\n",
    "            matrix = word_vec.fit_transform(input_dict)\n",
    "\n",
    "            results = model.predict([matrix.todense()])[0]\n",
    "            # print(results)\n",
    "            results_index = numpy.argmax(results)\n",
    "            tag = labels_dict[results_index]\n",
    "\n",
    "            if results[results_index] > 0.8:\n",
    "                for tg in dataset[\"data\"]:\n",
    "                    if tg['tag'] == tag:\n",
    "                        responses = tg['responses']\n",
    "\n",
    "                print(\"Bot: \"+random.choice(responses))\n",
    "            else:\n",
    "                print(\"Sorry, I dont understand\")\n",
    "\n",
    "\n",
    "lang = input(\"\\n Select language (english or albanian): \")\n",
    "\n",
    "# Load dataset\n",
    "with open('conversation_dataset/'+lang+'_dataset.json') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "try:\n",
    "    labels_dict, pattern_vocab = load_data(dataset, lang)\n",
    "    loaded_model = keras.models.load_model(\"keras_models/\"+lang+\"_model.h5\")\n",
    "    command = input(\"\\nModel already exists! Do you wanna chat? (y/n): \")\n",
    "\n",
    "    if command.lower() == 'y':\n",
    "        chat(loaded_model, labels_dict, pattern_vocab)\n",
    "except:\n",
    "    labels_dict, pattern_vocab, input_x, input_y = generate_data(dataset, lang)\n",
    "    model = create_model(input_x, input_y)\n",
    "    model.fit(input_x, input_y, epochs=500, batch_size=8)\n",
    "    model.save(\"keras_models/\"+lang+\"_model.h5\")\n",
    "    chat(model, labels_dict, pattern_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d6160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
